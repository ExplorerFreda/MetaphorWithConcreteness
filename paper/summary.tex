

% introduction (TBC)
\section{introduction}
Personalization, as well as comparing human to animals, is a ubiquitous phenomenon in natural languages. 
In such cases, each kind of animal has its specific reasons to be mentioned. 
For example, the sentence ``he is like a tiger'' is more likely to refer to ``he is strong'', rather than ``he is sly''. 
The reason of such fact is we have already kept a prototype of tiger in our mind: all tigers are strong, but they are not necessary to be sly. 
We call properties like strong, ferocious and brave ``central properties (or central attributes)'' of tigers. To be specific, if the sentence
``{\sl All N(s) are A.}''
is reasonable and common in a non-specific situation, we could treat A as a central property of N (there, N is a noun and A is an adjective).

The central properties of nouns can be applied to metaphor generation, making natural language generated by computer more vivid. It can be helpful to the collection of a metaphor knowledge base, in which each item could be stored as a triplet $(n_{source}, n_{target}, adj_{reason})$.  As a example, most people agree that fat can be has a central property of pigs, but not central one of cats. Therefore, we could have (cat, pig, fat) as an item in the knowledge base, with which computer could generate the sentence like ``the cat is like a pig'' to express ``the cat is fat''. Such generation can be interpreted easily. 

More generally, not only animals have their central properties, the collection of our knowledge base could be expand into other domains rather than animals. 

% related work (TBC)
\section{Related Work}

\newcite{veale2007learning} built an English knowledge base ``sardonicus'' with simile queries. 
\newcite{li2015chinese} have built a Chinese one and indicated that would shed light to metaphor related tasks.
\newcite{bulatmodelling} have shown that attributes (or properties) are helpful to improve computational metaphor modeling. 

For feature extraction, we used Linggle \cite{boisson2013linggle}. 
It provides an efficient API to rapidly query the frequency and collocations of words. 

% experiments (pipelines) 
\section{Experiments}

\subsection{Feature Collection}
To indicate whether an adjective is a central property of a noun, we apply the following features. 
We extracted the features below between 2,816 adjectives and 54 nouns. 

\subsubsection{Frequency and Collocation}
Word frequency and collocation are intuitive descriptors of a document. Linggle \cite{boisson2013linggle} makes it easy for us to extract a word's frequency and two words' collocation frequency in web-scale corpus. 
\subsubsection{PMI}
Pairwise mutual information (or point mutual information), known as PMI \cite{church1990word, bouma2009normalized}, is a widely accepted measure that illustrates the association between two words. 
The PMI between two words $x$ and $y$ can be computed by 
\begin{equation}
PMI(x, y) = \log_2 \frac{P(x,y)}{P(x)P(y)}
\end{equation}
where $P(x)$ is the probability of word $x$ to appear, 
$P(x,y)$ is the joint probability of $x$ and $y$, namely the probability of the collocation of $x$ and $y$. 
Based on the statistics, it can be estimated by the frequency of word $x$ and $y$,  with the collocation frequency of them. 
In application, it is usually computed by 
\begin{equation}
\begin{aligned}
PMI(x,y) &= \log_2 \frac{C(x,y) \cdot N}{C(x)C(y)}  \\
 &= \log_2{C(x,y)} - \log_2{C(x)} \\ 
 & \quad - \log_2{C(y)} + \log_2 N
\end{aligned}
\end{equation}
where $N$ is the number of unigrams in total, 
$C(x)$ and $C(y)$ represent the frequency of unigram $x$ and $y$, 
and $C(x,y)$ represents the frequency of bigram $(x, y)$. 
With no loss of generality, we omit the constant $\log_2 N$ and compute $PMI^{*}(x,y) = \log_2{C(x,y)} - \log_2{C(x)} - \log_2{C(y)}$ instead of $PMI(x,y)$.

\subsubsection{Concreteness of words}
\newcite{turney2011literal} proposed a method to compute the concreteness and abstractness of words, and suggested that concreteness of words are strong indicators of metaphors. 
We replicated the experiments for concreteness computation with the method on a large web scale corpus, and got a list with 75,626 words' concreteness.
We also treat concreteness as a feature for central attributes extraction.

\subsubsection{"As ... as" pattern}
Previous works have shown that the simile template ``as adjective as noun'' can be applied to extract properties of nouns \cite{roncero2006similes, veale2007learning, li2015chinese}.
\newcite{veale2007learning} also indicated that simile is widely viewed as a less sophisticated conceptual device than metaphor, and simile detection would be helpful to extract metaphors. 
Thus, the frequency of cooccurrence of an adjective and a noun pair in the pattern of "as adj. as a/an noun." would be a strong feature to detect central properties of nouns. 
We used Linggle API \cite{boisson2013linggle} \footnote{add the queries after the url {\sl http://linggle.com/query/}} to extract such patterns, with the queries like ``as strong as a/an n.''  and ``as adj. as a/an tiger'' , and it would return 50 most frequent matched phrases.



\subsection{Annotation}  %(TBC)


% future work 

\section{Future Work}
\begin{itemize}
\item The concept of central property should not be limited in only adjectives. For example, ``have sharp teeth'' should be a central property of tigers or lions, so that something could be compared to them in order to indicate such a property.
\end{itemize}